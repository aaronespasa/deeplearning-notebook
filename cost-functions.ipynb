{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cost-functions.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "VPNIU9Nk9vWW",
        "DvGz45yz8CD5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyYlf2Hl34aw"
      },
      "source": [
        "# Función de coste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvU6hjbx36Cu"
      },
      "source": [
        "Es una medida de que tan equivocado está nuestro modelo en términos de su habilidad para estimar la relación entre X e y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_9dYrwl4P2k"
      },
      "source": [
        "La función de coste es un solo valor, no un vector. Esto es debido a que evalua qué tal actúa la red neuronal como un todo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuJUTCAv6VT8"
      },
      "source": [
        "# Tipos de funciones de coste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoHwKjw66X1u"
      },
      "source": [
        "Se dividen en 2 tipos:\n",
        "- Funciones de coste para regresión.\n",
        "- Funciones de coste para clasificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HibkLPC6ipY"
      },
      "source": [
        "## 1.Funciones de coste para regresión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl8Vthj96noi"
      },
      "source": [
        "Considerando:\n",
        "\n",
        "$Y$ - Output real\n",
        "\n",
        "$\\hat{Y}$ - Output predecido\n",
        "\n",
        "Las funciones de coste en regresión son calculadas como errores basados en distancia:\n",
        "\n",
        "$$Error = y - \\hat{y}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKoiGF9x97OP"
      },
      "source": [
        "Considerando `n` el número total de data points:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPNIU9Nk9vWW"
      },
      "source": [
        "### 1.1 Mean Error (ME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjP3ehq49zKZ"
      },
      "source": [
        "$$ME = \\frac{1}{n} * \\sum{(y - \\hat{y})}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqMRnElW-Bam"
      },
      "source": [
        "Los errores pueden ser tanto positivos como negativos. Esto haría que se cancelasen en la sumatoria, dando lugar a un modelo con media de error 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JKsTQpx-W2h"
      },
      "source": [
        "Por esto mismo, no es recomendado.\n",
        "\n",
        "Sin embargo, sirve como fundamento para las siguientes funciones de coste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ReA7BU-clX"
      },
      "source": [
        "### 1.2 Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZm-cenr-iuC"
      },
      "source": [
        "$$MSE = \\frac{1}{n} * \\sum{(y - \\hat{y})^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJkLhSSf-zMW"
      },
      "source": [
        "El cuadrado evita la existencia de valores negativos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry7VI99d_ZQn"
      },
      "source": [
        "A este también se le conoce como el **L2 Loss**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSvdaZsv-9xn"
      },
      "source": [
        "Debido a que cada error está al cuadrado, esto ayuda a penalizar incluso las pequeñas desviaciones en comparación con el ME.\n",
        "\n",
        "Sin embargo, esto hace que sea **menos robusto ante outliers** porque aumentaría aún más el error ante estos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvGz45yz8CD5"
      },
      "source": [
        "### 1.3 Mean Absolute Error (MAE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bexfHVwv8Iv2"
      },
      "source": [
        "$$MAE = \\frac{1}{n} * \\sum{|y - \\hat{y}|}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7itNVjS9SHx"
      },
      "source": [
        "A este también se le conoce como el **L1 Loss**.\n",
        "\n",
        "**Funciona bien ante la presencia de outliers**, así que obtendremos buenos resultados aunque nuestro dataset tenga ruido o outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpEu9DeSANa0"
      },
      "source": [
        "### 1.4 Root Mean Squared Error (RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9OXq5R9AUDZ"
      },
      "source": [
        "La distancia euclediana entre dos puntos se representa de la siguiente forma:\n",
        "\n",
        "$$distance(x,y) = \\sqrt{\\sum^n_{i=1}{(x_i - y_i)^2}}$$\n",
        "\n",
        "El RMSE puede entenderse como una normalización en la distancia euclediana entre el vector de los valores predecidos y el vector de los valores reales:\n",
        "\n",
        "$$RMSE = \\sqrt{\\sum^n_{i=1}{\\frac{(y_i - \\hat{y_i})^2}{n}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBWDCQLhBn7n"
      },
      "source": [
        "El problema de esta función de coste es que es sensible ante outliers. Aún así, la penalización es menor que en el MSE por la raíz cuadrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATx6Rd8NBkMb"
      },
      "source": [
        "### 1.5 Root Mean Squared Logarithmic Error (RMSLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqtNqpLICIHz"
      },
      "source": [
        "RMSLE es menos sensible ante outliers que la RMSE. Esto se debe a que disminuye la penalización de grandes errores por el uso del logaritmo aplicado antes de calcular la diferencia entre valores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiYmhKm4CdxA"
      },
      "source": [
        "$$RMSLE = \\sqrt{\\frac{1}{n} * \\sum^n_{i=1}{[log(y_i + 1) - log(\\hat{y_i} + 1)]}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwQbfpMy6oeA"
      },
      "source": [
        "## 2.Funciones de coste para clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8orIf7hn6qT0"
      },
      "source": [
        "### 2.1 Binary Cross Entropy Cost Function (Log Loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8unpjFkbD682"
      },
      "source": [
        "En este caso, $y_i$ puede ser 0 (primera clase) o 1 (segunda clase). \n",
        "\n",
        "Si $y_i$ es 1, buscaremos conseguir $-log(p) = -y * log(p)$.\n",
        "\n",
        "Si $y_i$ es 0, buscaremos conseguir $-log(1 - p) = - (1 - y) * log(1 - p)$.\n",
        "\n",
        "Si unimos estas dos, obtendremos\n",
        "\n",
        "$$-y * log(p) - (1 - y) * log(1 - p)$$\n",
        "\n",
        "Y una vez normalizado para todos los data points:\n",
        "\n",
        "$$CrossEntropy = -\\frac{1}{n} * \\sum^n_{i=1}{[-y * log(p) - (1 - y) * log(1 - p)]} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fQbQmL5F3ej"
      },
      "source": [
        "Para aprender más sobre esta función de coste, recomiendo el siguiente artículo:\n",
        "\n",
        "[Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndCyLgiIG5Cz"
      },
      "source": [
        "### 2.2 Multi-class Cross Entropy Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUNjkD5FG_gX"
      },
      "source": [
        "Podemos utilizar la misma cross entropy que en 2.1, utilizando una función de activación sigmoide en la última capa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKTZWw6c4cZ0"
      },
      "source": [
        "# Recursos:\n",
        "- [Lista de las funciones de coste en redes neuronales y su aplicación](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)\n",
        "- [Clasificación de las funciones de coste](https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUSwevYQ6_f4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}